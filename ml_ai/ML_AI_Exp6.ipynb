{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spam.csv',encoding='latin1')\n",
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1\n",
       "ham     4828\n",
       "spam     751\n",
       "Name: v2, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(data['v1']).count()['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_spam = list(data.loc[data.v1 == 'spam']['v2'])\n",
    "# train_ham = list(data.loc[data.v1 == 'ham']['v2'])\n",
    "# Training data for naive bayes text classifier\n",
    "train_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "train_ham = ['Your activity report','benefits physical activity', 'the importance vows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'account']\n",
      "['Your', 'activity', 'report', 'benefits', 'physical', 'the', 'importance', 'vows']\n"
     ]
    }
   ],
   "source": [
    "# Make a vocabulary of unique words that occur in known spam emails\n",
    "vocab_words_spam = []\n",
    "\n",
    "for sentence in train_spam:\n",
    "    sentence_as_list = sentence.split()\n",
    "    for word in sentence_as_list:\n",
    "        vocab_words_spam.append(word)\n",
    "        \n",
    "vocab_unique_words_spam = list(dict.fromkeys(vocab_words_spam))\n",
    "print(vocab_unique_words_spam)\n",
    "print(vocab_unique_words_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails with the word send: 3\n",
      "Spamicity of the word 'send': 0.6666666666666666 \n",
      "\n",
      "Number of spam emails with the word us: 2\n",
      "Spamicity of the word 'us': 0.5 \n",
      "\n",
      "Number of spam emails with the word your: 3\n",
      "Spamicity of the word 'your': 0.6666666666666666 \n",
      "\n",
      "Number of spam emails with the word password: 2\n",
      "Spamicity of the word 'password': 0.5 \n",
      "\n",
      "Number of spam emails with the word review: 1\n",
      "Spamicity of the word 'review': 0.3333333333333333 \n",
      "\n",
      "Number of spam emails with the word our: 4\n",
      "Spamicity of the word 'our': 0.8333333333333334 \n",
      "\n",
      "Number of spam emails with the word website: 1\n",
      "Spamicity of the word 'website': 0.3333333333333333 \n",
      "\n",
      "Number of spam emails with the word account: 1\n",
      "Spamicity of the word 'account': 0.3333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating probability of words occuring in spam emails\n",
    "dict_spamicity = {}\n",
    "for w in vocab_unique_words_spam:\n",
    "    emails_with_w = 0 \n",
    "    for sentence in train_spam:\n",
    "        if w in sentence:\n",
    "            emails_with_w+=1\n",
    "            \n",
    "    print(f\"Number of spam emails with the word {w}: {emails_with_w}\")\n",
    "    total_spam = len(train_spam)\n",
    "    spamicity = (emails_with_w+1)/(total_spam+2)\n",
    "    print(f\"Spamicity of the word '{w}': {spamicity} \\n\")\n",
    "    dict_spamicity[w.lower()] = spamicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a vocabulary of unique words that occur in known ham emails\n",
    "vocab_words_ham = []\n",
    "for sentence in train_ham:\n",
    "    sentence_as_list = sentence.split()\n",
    "    for word in sentence_as_list:\n",
    "        vocab_words_ham.append(word)\n",
    "        \n",
    "vocab_unique_words_ham = list(dict.fromkeys(vocab_words_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ham emails with the word 'Your': 1\n",
      "Hamicity of the word 'Your': 0.4 \n",
      "Number of ham emails with the word 'activity': 2\n",
      "Hamicity of the word 'activity': 0.6 \n",
      "Number of ham emails with the word 'report': 1\n",
      "Hamicity of the word 'report': 0.4 \n",
      "Number of ham emails with the word 'benefits': 1\n",
      "Hamicity of the word 'benefits': 0.4 \n",
      "Number of ham emails with the word 'physical': 1\n",
      "Hamicity of the word 'physical': 0.4 \n",
      "Number of ham emails with the word 'the': 1\n",
      "Hamicity of the word 'the': 0.4 \n",
      "Number of ham emails with the word 'importance': 1\n",
      "Hamicity of the word 'importance': 0.4 \n",
      "Number of ham emails with the word 'vows': 1\n",
      "Hamicity of the word 'vows': 0.4 \n"
     ]
    }
   ],
   "source": [
    "# Calculating probability of words occuring in ham emails\n",
    "dict_hamicity = {}\n",
    "for w in vocab_unique_words_ham:\n",
    "    emails_with_w = 0  \n",
    "    for sentence in train_ham:\n",
    "        if w in sentence:\n",
    "            emails_with_w += 1\n",
    "            \n",
    "    print(f\"Number of ham emails with the word '{w}': {emails_with_w}\")\n",
    "    total_ham = len(train_ham)\n",
    "    Hamicity = (emails_with_w+1)/(total_ham+2)      \n",
    "    print(f\"Hamicity of the word '{w}': {Hamicity} \")\n",
    "    dict_hamicity[w.lower()] = Hamicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Spam:  0.5714285714285714\n",
      "Probability of Ham:  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "prob_spam = len(train_spam) / (len(train_spam)+(len(train_ham)))\n",
    "print('Probability of Spam: ',prob_spam)\n",
    "prob_ham = len(train_ham) / (len(train_spam)+(len(train_ham)))\n",
    "print('Probability of Ham: ',prob_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['renew', 'your', 'password'], ['renew', 'your', 'vows']]\n"
     ]
    }
   ],
   "source": [
    "# Using the split data to test the algorithm\n",
    "test_emails = {'spam':['renew your password', 'renew your vows','hello'], \n",
    "               'ham':['benefits of our account', 'the importance of physical activity']}\n",
    "tests = []\n",
    "for i in test_emails['spam']:\n",
    "    tests.append(i)\n",
    "    \n",
    "for i in test_emails['ham']:\n",
    "    tests.append(i)\n",
    "\n",
    "# split emails into distinct words\n",
    "distinct_words_as_sentences_test = []\n",
    "for sentence in tests:\n",
    "    sentence_as_list = sentence.split()\n",
    "    senten = []\n",
    "    for word in sentence_as_list:\n",
    "        senten.append(word)\n",
    "    distinct_words_as_sentences_test.append(senten)\n",
    "\n",
    "test_spam_tokenized = [distinct_words_as_sentences_test[0], distinct_words_as_sentences_test[1]]\n",
    "test_ham_tokenized = [distinct_words_as_sentences_test[2], distinct_words_as_sentences_test[3]]\n",
    "print(test_spam_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing for spam:\n",
      "'renew', word not present in labelled spam training data\n",
      "'your', ok\n",
      "'password', ok\n",
      "'renew', word not present in labelled spam training data\n",
      "'your', ok\n",
      "'vows', ok\n",
      "[['your', 'password'], ['your', 'vows']]\n",
      "\n",
      "Reducing for ham:\n",
      "'hello', word not present in labelled ham training data\n",
      "'benefits', ok\n",
      "'of', word not present in labelled ham training data\n",
      "'our', ok\n",
      "'account', ok\n",
      "[[], ['benefits', 'our', 'account']]\n"
     ]
    }
   ],
   "source": [
    "# Reducing dataset based on test data\n",
    "print('Reducing for spam:')\n",
    "reduced_sentences_spam_test = []\n",
    "for sentence in test_spam_tokenized:\n",
    "    words_ = []\n",
    "    for word in sentence:\n",
    "        if word in vocab_unique_words_spam:\n",
    "            print(f\"'{word}', ok\")\n",
    "            words_.append(word)\n",
    "        elif word in vocab_unique_words_ham:\n",
    "            print(f\"'{word}', ok\")\n",
    "            words_.append(word)\n",
    "        else:\n",
    "            print(f\"'{word}', word not present in labelled spam training data\")\n",
    "    reduced_sentences_spam_test.append(words_)\n",
    "print(reduced_sentences_spam_test)\n",
    "\n",
    "print('\\nReducing for ham:')\n",
    "reduced_sentences_ham_test = []                   \n",
    "for sentence in test_ham_tokenized:\n",
    "    words_ = []\n",
    "    for word in sentence:\n",
    "        if word in vocab_unique_words_ham:\n",
    "            print(f\"'{word}', ok\")\n",
    "            words_.append(word)\n",
    "        elif word in vocab_unique_words_spam:\n",
    "            print(f\"'{word}', ok\")\n",
    "            words_.append(word)\n",
    "        else:\n",
    "            print(f\"'{word}', word not present in labelled ham training data\")\n",
    "    reduced_sentences_ham_test.append(words_)\n",
    "print(reduced_sentences_ham_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove\n",
      "remove\n",
      "[['password'], ['vows']]\n"
     ]
    }
   ],
   "source": [
    "test_spam_stemmed = []\n",
    "non_key = ['us','the', 'of','your']       # non-key words, gathered from spam,ham and test sentences\n",
    "for email in reduced_sentences_spam_test:\n",
    "    email_stemmed=[]\n",
    "    for word in email:\n",
    "        if word in non_key:\n",
    "            print('remove')\n",
    "        else:\n",
    "            email_stemmed.append(word)\n",
    "    test_spam_stemmed.append(email_stemmed)\n",
    "            \n",
    "print(test_spam_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ham_stemmed = []\n",
    "non_key = ['us',  'the', 'of', 'your'] \n",
    "for email in reduced_sentences_ham_test:\n",
    "    email_stemmed=[]\n",
    "    for word in email:\n",
    "        if word in non_key:\n",
    "            print('remove')\n",
    "        else:\n",
    "            email_stemmed.append(word)\n",
    "    test_ham_stemmed.append(email_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing stemmed SPAM email ['password'] :\n",
      "Test word by word: \n",
      "prob of spam in general  0.5714285714285714\n",
      "prob \"password\"  is a spam word : 0.5\n",
      "prob of ham in general  0.42857142857142855\n",
      "WH for password is 0.2\n",
      "prob 'password' is a ham word: 0.2\n",
      "\n",
      "Using Bayes, prob the the word 'password' is spam: 0.7692307692307692\n",
      "###########################\n",
      "All word probabilities for this sentence: [0.7692307692307692]\n",
      "email is SPAM: with spammy confidence of 76.92307692307692%\n",
      "0.7692307692307692\n",
      "\n",
      "Testing stemmed SPAM email ['vows'] :\n",
      "Test word by word: \n",
      "prob of spam in general  0.5714285714285714\n",
      "prob 'vows' is a spam word: 0.16666666666666666\n",
      "prob of ham in general  0.42857142857142855\n",
      "prob \"vows\" is a ham word:  0.4\n",
      "\n",
      "Using Bayes, prob the the word 'vows' is spam: 0.35714285714285715\n",
      "###########################\n",
      "All word probabilities for this sentence: [0.35714285714285715]\n",
      "email is HAM: with spammy confidence of 35.714285714285715%\n",
      "0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "def mult(list_) :        # function to multiply all word probs together \n",
    "    total_prob = 1\n",
    "    for i in list_: \n",
    "         total_prob = total_prob * i  \n",
    "    return total_prob\n",
    "\n",
    "def Bayes(email):\n",
    "    probs = []\n",
    "    for word in email:\n",
    "        Pr_S = prob_spam\n",
    "        print('prob of spam in general ',Pr_S)\n",
    "        try:\n",
    "            pr_WS = dict_spamicity[word]\n",
    "            print(f'prob \"{word}\"  is a spam word : {pr_WS}')\n",
    "        except KeyError:\n",
    "            pr_WS = 1/(total_spam+2)  # Apply smoothing for word not seen in spam training data, but seen in ham training \n",
    "            print(f\"prob '{word}' is a spam word: {pr_WS}\")\n",
    "            \n",
    "        Pr_H = prob_ham\n",
    "        print('prob of ham in general ', Pr_H)\n",
    "        try:\n",
    "            pr_WH = dict_hamicity[word]\n",
    "            print(f'prob \"{word}\" is a ham word: ',pr_WH)\n",
    "        except KeyError:\n",
    "            pr_WH = (1/(total_ham+2))  # Apply smoothing for word not seen in ham training data, but seen in spam training\n",
    "            print(f\"WH for {word} is {pr_WH}\")\n",
    "            print(f\"prob '{word}' is a ham word: {pr_WH}\")\n",
    "        \n",
    "        prob_word_is_spam_BAYES = (pr_WS*Pr_S)/((pr_WS*Pr_S)+(pr_WH*Pr_H))\n",
    "        print('')\n",
    "        print(f\"Using Bayes, prob the the word '{word}' is spam: {prob_word_is_spam_BAYES}\")\n",
    "        print('###########################')\n",
    "        probs.append(prob_word_is_spam_BAYES)\n",
    "    print(f\"All word probabilities for this sentence: {probs}\")\n",
    "    final_classification = mult(probs)\n",
    "    if final_classification >= 0.5:\n",
    "        print(f'email is SPAM: with spammy confidence of {final_classification*100}%')\n",
    "    else:\n",
    "        print(f'email is HAM: with spammy confidence of {final_classification*100}%')\n",
    "    return final_classification\n",
    "for email in test_spam_stemmed:\n",
    "    print(f\"\\nTesting stemmed SPAM email {email} :\")\n",
    "    print('Test word by word: ')\n",
    "    all_word_probs = Bayes(email)\n",
    "    print(all_word_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing stemmed HAM email [] :\n",
      "Test word by word: \n",
      "All word probabilities for this sentence: []\n",
      "email is SPAM: with spammy confidence of 100%\n",
      "1\n",
      "\n",
      "Testing stemmed HAM email ['benefits', 'our', 'account'] :\n",
      "Test word by word: \n",
      "prob of spam in general  0.5714285714285714\n",
      "prob 'benefits' is a spam word: 0.16666666666666666\n",
      "prob of ham in general  0.42857142857142855\n",
      "prob \"benefits\" is a ham word:  0.4\n",
      "\n",
      "Using Bayes, prob the the word 'benefits' is spam: 0.35714285714285715\n",
      "###########################\n",
      "prob of spam in general  0.5714285714285714\n",
      "prob \"our\"  is a spam word : 0.8333333333333334\n",
      "prob of ham in general  0.42857142857142855\n",
      "WH for our is 0.2\n",
      "prob 'our' is a ham word: 0.2\n",
      "\n",
      "Using Bayes, prob the the word 'our' is spam: 0.847457627118644\n",
      "###########################\n",
      "prob of spam in general  0.5714285714285714\n",
      "prob \"account\"  is a spam word : 0.3333333333333333\n",
      "prob of ham in general  0.42857142857142855\n",
      "WH for account is 0.2\n",
      "prob 'account' is a ham word: 0.2\n",
      "\n",
      "Using Bayes, prob the the word 'account' is spam: 0.689655172413793\n",
      "###########################\n",
      "All word probabilities for this sentence: [0.35714285714285715, 0.847457627118644, 0.689655172413793]\n",
      "email is HAM: with spammy confidence of 20.873340569424727%\n",
      "0.20873340569424728\n"
     ]
    }
   ],
   "source": [
    "for email in test_ham_stemmed:\n",
    "    print(f\"\\nTesting stemmed HAM email {email} :\")\n",
    "    print('Test word by word: ')\n",
    "    all_word_probs = Bayes(email)\n",
    "    print(all_word_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
